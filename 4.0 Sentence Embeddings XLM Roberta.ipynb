{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-06-06T08:52:11.041534Z",
     "start_time": "2025-06-06T08:52:11.027282Z"
    }
   },
   "source": [
    "from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
    "import pandas as pd\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "import torch\n",
    "from torch import nn\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "import time\n",
    "from torch.optim import AdamW"
   ],
   "outputs": [],
   "execution_count": 33
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Config",
   "id": "20490e701af3b1ce"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-05T18:43:40.819717Z",
     "start_time": "2025-06-05T18:43:40.817778Z"
    }
   },
   "cell_type": "code",
   "source": [
    "learning_rate = 5e-6\n",
    "n_epochs = 2\n",
    "gradient_acc = 8\n",
    "batch_size = 4\n",
    "max_len = 512\n",
    "seed = 8824\n",
    "weight_decay = 1e-4\n",
    "warmup_rate = 0.1\n",
    "overall_weight = 0.75\n",
    "rdrop_weight = 0.1\n",
    "\n",
    "model_save_path = \"saved_models/best_mmregressor.pth\""
   ],
   "id": "c233f5f170e5f153",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-05T18:43:41.870248Z",
     "start_time": "2025-06-05T18:43:40.847522Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"FacebookAI/xlm-roberta-base\")\n",
    "device = torch.device(\"cpu\")"
   ],
   "id": "69fec0aca4f0c6c",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Dataloader",
   "id": "5068d5b973a745cb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-05T18:43:52.812020Z",
     "start_time": "2025-06-05T18:43:41.894164Z"
    }
   },
   "cell_type": "code",
   "source": [
    "input_ids, attention_masks, labels = [], [], []\n",
    "\n",
    "# Load and tokenize the dataset\n",
    "for idx, row in pd.read_csv(\"data/full_dataset.csv\").iterrows():\n",
    "    text1 = str(row['text1']) if pd.notna(row['text1']) else \"\"\n",
    "    text2 = str(row['text2']) if pd.notna(row['text2']) else \"\"\n",
    "\n",
    "    encode_dict = tokenizer(\n",
    "        text1,\n",
    "        text2,\n",
    "        max_length=max_len,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        add_special_tokens=True\n",
    "    )\n",
    "\n",
    "    input_ids.append(encode_dict['input_ids'])\n",
    "    attention_masks.append(encode_dict['attention_mask'])\n",
    "    labels.append([\n",
    "        float(x) for x in [\n",
    "            row['geography'], row['entities'], row['time'],\n",
    "            row['narrative'], row['overall'], row['style'], row['tone']\n",
    "        ]\n",
    "    ])\n",
    "\n",
    "# Sanity check\n",
    "print(\"Input IDs:\", input_ids[0])\n",
    "print(\"Attention Mask:\", attention_masks[0])\n",
    "print(\"Labels:\", labels[0])\n",
    "\n",
    "# Split the data before converting to tensors\n",
    "train_inputs, test_inputs, train_masks, test_masks, train_labels, test_labels = train_test_split(\n",
    "    input_ids, attention_masks, labels, test_size=0.2, random_state=seed\n",
    ")\n",
    "\n",
    "# Convert to tensors\n",
    "train_inputs = torch.tensor(train_inputs)\n",
    "test_inputs = torch.tensor(test_inputs)\n",
    "train_masks = torch.tensor(train_masks)\n",
    "test_masks = torch.tensor(test_masks)\n",
    "train_labels = torch.tensor(train_labels, dtype=torch.float)\n",
    "test_labels = torch.tensor(test_labels, dtype=torch.float)\n",
    "\n",
    "# Create Datasets\n",
    "train_dataset = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "test_dataset = TensorDataset(test_inputs, test_masks, test_labels)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, drop_last=False)\n"
   ],
   "id": "16b45acc72bc280d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs: [0, 162607, 85717, 15639, 48802, 4, 601, 5, 28563, 5, 292, 62, 92610, 297, 104, 79099, 33, 27941, 509, 34784, 297, 23, 10, 2356, 42552, 26, 7, 10013, 11192, 7514, 150631, 450, 152388, 15700, 2926, 1419, 23, 10542, 118623, 4, 35206, 2804, 5, 581, 27998, 74918, 2822, 39395, 64227, 33233, 127067, 99091, 5337, 61340, 16503, 23, 2076, 350, 4293, 47064, 4, 581, 34419, 113771, 5, 1311, 1459, 3316, 112, 335, 104158, 634, 1486, 4, 2789, 4, 111, 17686, 104588, 4, 118623, 4, 509, 25534, 71, 678, 14614, 568, 678, 47219, 4, 18738, 13, 214, 14614, 568, 4, 136, 8035, 10, 63323, 3674, 3445, 23, 139355, 111, 10, 11476, 56859, 136, 10, 116131, 4935, 1295, 87338, 4, 2076, 350, 4293, 47064, 53257, 32920, 17065, 1814, 161644, 2804, 5, 161644, 2804, 10, 8, 7077, 53, 32603, 1916, 11782, 7, 1672, 10, 6, 115656, 9393, 27941, 81887, 297, 47, 3249, 10, 83629, 7279, 4, 1284, 70, 92610, 26, 7, 80939, 13986, 71, 1257, 136, 5962, 10, 2258, 4, 3129, 7068, 91, 173964, 10, 56050, 68823, 5, 581, 27941, 111, 70, 2258, 509, 97160, 309, 37534, 103494, 99, 70, 28302, 5, 581, 27941, 26, 7, 9351, 509, 959, 109312, 121447, 5, 581, 92610, 26, 7, 80939, 2843, 5962, 10, 8, 7077, 53, 26, 7, 149509, 42, 8108, 8035, 118066, 4, 161644, 2804, 5, 335, 104158, 634, 1486, 509, 8035, 34658, 15490, 71052, 23, 70, 24453, 9022, 28379, 823, 379, 5, 1650, 58954, 26, 18, 109312, 51529, 36766, 335, 104158, 634, 1486, 1556, 142, 200866, 5, 6, 116185, 32007, 1295, 2, 2, 122390, 9, 20000, 9, 683, 65443, 10854, 4, 4368, 118, 292, 4368, 55283, 34202, 124445, 8876, 2501, 9392, 184, 7155, 350, 678, 40250, 98, 64227, 136, 176016, 71, 70, 23295, 26, 7, 23, 215131, 5155, 23, 70, 10323, 100, 48031, 89397, 25632, 21775, 111, 68894, 2130, 432, 379, 5, 2501, 9392, 184, 4, 124901, 27759, 1556, 2809, 61689, 5281, 111, 6, 66157, 1830, 4, 168, 85018, 71, 26603, 18, 20271, 1919, 116483, 99, 70, 9907, 73416, 23, 13791, 9, 916, 9, 112079, 329, 136, 104687, 71, 4368, 118, 26, 7, 100307, 47, 4488, 678, 70, 27759, 136, 4358, 28282, 188513, 5, 52, 12137, 26, 107, 7464, 111531, 70425, 4, 63, 764, 2804, 5, 52, 20800, 8364, 2750, 21342, 47, 2046, 98870, 7413, 442, 3638, 450, 1836, 54, 959, 8783, 152838, 4, 7413, 442, 3638, 450, 2685, 831, 186, 110, 130412, 4, 7413, 442, 3638, 450, 1836, 5423, 112034, 100, 107222, 7, 4, 41866, 3229, 903, 107222, 83, 70, 11341, 68034, 974, 2501, 9392, 184, 2843, 1747, 14071, 29367, 100, 70, 23295, 26, 7, 221560, 14537, 628, 92674, 136, 75312, 24243, 1919, 780, 80399, 915, 181606, 47, 22691, 39108, 2481, 744, 21974, 10, 5155, 4, 54433, 442, 509, 182, 820, 47, 163846, 3501, 764, 28987, 71, 5, 581, 116483, 450, 94419, 70, 105150, 927, 204320, 111, 70, 8999, 26, 7, 5117, 22556, 456, 57964, 509, 7311, 538, 14865, 3674, 47, 5646, 3687, 23, 70, 144477, 9022, 184085, 289, 59444, 111, 2016, 76, 60877, 4, 7440, 16162, 9, 6979, 238, 7162, 65432, 59801, 159978, 71, 4368, 2]\n",
      "Attention Mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Labels: [4.0, 4.0, 1.0, 4.0, 4.0, 1.6666666666666667, 2.0]\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Model",
   "id": "78d854a436c8b2cf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-05T18:43:52.839374Z",
     "start_time": "2025-06-05T18:43:52.837167Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class MMRegressor(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        super(MMRegressor, self).__init__()\n",
    "        self.config = AutoConfig.from_pretrained(\"FacebookAI/xlm-roberta-base\")\n",
    "        self.reg_model = AutoModel.from_pretrained(\"FacebookAI/xlm-roberta-base\")\n",
    "\n",
    "        self.fc1 = nn.Linear(self.config.hidden_size, 512)\n",
    "        self.fc2 = nn.Linear(512, 7)\n",
    "        self.activation = nn.GELU()\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "\n",
    "        output = self.reg_model(input_ids, attention_mask)[1]\n",
    "        logits = self.fc2(self.activation(self.fc1(output)))\n",
    "\n",
    "        return logits"
   ],
   "id": "4e14b9be075ccf66",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Train & Evaluate",
   "id": "86e8845bc99e89d9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-05T18:46:54.567981Z",
     "start_time": "2025-06-05T18:46:54.564200Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_and_evaluate(model, train_loader, val_loader, device):\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    total_steps = len(train_loader) * n_epochs\n",
    "    scheduler = get_cosine_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=int(warmup_rate * total_steps),\n",
    "        num_training_steps=total_steps\n",
    "    )\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        start_time = time.time()\n",
    "        train_loss = 0.0\n",
    "\n",
    "        print(f\"—————————————————————— Epoch {epoch+1} ——————————————————————\")\n",
    "\n",
    "        for step, batch in enumerate(train_loader, 1):\n",
    "            input_ids, attention_mask, labels = [x.to(device) for x in batch]\n",
    "\n",
    "            outputs = model(input_ids, attention_mask)  # Assume model returns [batch_size, 7]\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            # Print progress every 10 batches\n",
    "            if step % 10 == 0 or step == len(train_loader):\n",
    "                print(f\"Epoch {epoch+1} [{step}/{len(train_loader)}] \"\n",
    "                      f\"Loss: {loss.item():.4f} \"\n",
    "                      f\"Progress: {100 * step / len(train_loader):.1f}%\")\n",
    "\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        elapsed = time.time() - start_time\n",
    "        minutes = int(elapsed // 60)\n",
    "        seconds = int(elapsed % 60)\n",
    "        print(f\"[Epoch {epoch+1}] Train Loss: {avg_train_loss:.4f} | Time: {minutes}m {seconds}s\")\n",
    "\n",
    "        # Evaluate\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        all_preds = []\n",
    "        all_targets = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                input_ids, attention_mask, labels = [x.to(device) for x in batch]\n",
    "                outputs = model(input_ids, attention_mask)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                all_preds.extend(outputs.cpu().numpy())\n",
    "                all_targets.extend(labels.cpu().numpy())\n",
    "\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        mae = mean_absolute_error(all_targets, all_preds)\n",
    "        r2 = r2_score(all_targets, all_preds)\n",
    "\n",
    "        print(f\"[Epoch {epoch+1}] Val Loss (MSE): {avg_val_loss:.4f}\")\n",
    "        print(f\"[Epoch {epoch+1}] Val MAE: {mae:.4f}\")\n",
    "        print(f\"[Epoch {epoch+1}] Val R²: {r2:.4f}\")\n",
    "        model.train()\n",
    "\n",
    "    # Save final model\n",
    "    torch.save(model.state_dict(), model_save_path)\n"
   ],
   "id": "7c79b059a589aa38",
   "outputs": [],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-05T19:45:57.538052Z",
     "start_time": "2025-06-05T18:47:01.247194Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = MMRegressor()\n",
    "train_and_evaluate(model, train_loader, test_loader, device)"
   ],
   "id": "24efb373a617ac6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "—————————————————————— Epoch 1 ——————————————————————\n",
      "Epoch 1 [10/547] Loss: 6.3783 Progress: 1.8%\n",
      "Epoch 1 [20/547] Loss: 4.0789 Progress: 3.7%\n",
      "Epoch 1 [30/547] Loss: 8.6415 Progress: 5.5%\n",
      "Epoch 1 [40/547] Loss: 6.9946 Progress: 7.3%\n",
      "Epoch 1 [50/547] Loss: 7.5339 Progress: 9.1%\n",
      "Epoch 1 [60/547] Loss: 6.6879 Progress: 11.0%\n",
      "Epoch 1 [70/547] Loss: 5.3838 Progress: 12.8%\n",
      "Epoch 1 [80/547] Loss: 6.1982 Progress: 14.6%\n",
      "Epoch 1 [90/547] Loss: 6.5797 Progress: 16.5%\n",
      "Epoch 1 [100/547] Loss: 1.6912 Progress: 18.3%\n",
      "Epoch 1 [110/547] Loss: 7.8363 Progress: 20.1%\n",
      "Epoch 1 [120/547] Loss: 5.9835 Progress: 21.9%\n",
      "Epoch 1 [130/547] Loss: 2.8989 Progress: 23.8%\n",
      "Epoch 1 [140/547] Loss: 2.0771 Progress: 25.6%\n",
      "Epoch 1 [150/547] Loss: 3.8994 Progress: 27.4%\n",
      "Epoch 1 [160/547] Loss: 2.1883 Progress: 29.3%\n",
      "Epoch 1 [170/547] Loss: 2.6699 Progress: 31.1%\n",
      "Epoch 1 [180/547] Loss: 3.1015 Progress: 32.9%\n",
      "Epoch 1 [190/547] Loss: 3.6091 Progress: 34.7%\n",
      "Epoch 1 [200/547] Loss: 3.6366 Progress: 36.6%\n",
      "Epoch 1 [210/547] Loss: 0.9951 Progress: 38.4%\n",
      "Epoch 1 [220/547] Loss: 1.0852 Progress: 40.2%\n",
      "Epoch 1 [230/547] Loss: 0.7706 Progress: 42.0%\n",
      "Epoch 1 [240/547] Loss: 2.0432 Progress: 43.9%\n",
      "Epoch 1 [250/547] Loss: 3.4877 Progress: 45.7%\n",
      "Epoch 1 [260/547] Loss: 2.2300 Progress: 47.5%\n",
      "Epoch 1 [270/547] Loss: 3.2601 Progress: 49.4%\n",
      "Epoch 1 [280/547] Loss: 1.5006 Progress: 51.2%\n",
      "Epoch 1 [290/547] Loss: 1.1172 Progress: 53.0%\n",
      "Epoch 1 [300/547] Loss: 0.8892 Progress: 54.8%\n",
      "Epoch 1 [310/547] Loss: 2.1017 Progress: 56.7%\n",
      "Epoch 1 [320/547] Loss: 1.7462 Progress: 58.5%\n",
      "Epoch 1 [330/547] Loss: 1.0283 Progress: 60.3%\n",
      "Epoch 1 [340/547] Loss: 0.5079 Progress: 62.2%\n",
      "Epoch 1 [350/547] Loss: 1.6485 Progress: 64.0%\n",
      "Epoch 1 [360/547] Loss: 0.8188 Progress: 65.8%\n",
      "Epoch 1 [370/547] Loss: 0.3897 Progress: 67.6%\n",
      "Epoch 1 [380/547] Loss: 1.1362 Progress: 69.5%\n",
      "Epoch 1 [390/547] Loss: 1.0842 Progress: 71.3%\n",
      "Epoch 1 [400/547] Loss: 0.8545 Progress: 73.1%\n",
      "Epoch 1 [410/547] Loss: 0.8198 Progress: 75.0%\n",
      "Epoch 1 [420/547] Loss: 0.8338 Progress: 76.8%\n",
      "Epoch 1 [430/547] Loss: 1.2656 Progress: 78.6%\n",
      "Epoch 1 [440/547] Loss: 1.0447 Progress: 80.4%\n",
      "Epoch 1 [450/547] Loss: 1.4950 Progress: 82.3%\n",
      "Epoch 1 [460/547] Loss: 0.9297 Progress: 84.1%\n",
      "Epoch 1 [470/547] Loss: 1.3606 Progress: 85.9%\n",
      "Epoch 1 [480/547] Loss: 1.0492 Progress: 87.8%\n",
      "Epoch 1 [490/547] Loss: 0.8786 Progress: 89.6%\n",
      "Epoch 1 [500/547] Loss: 0.9533 Progress: 91.4%\n",
      "Epoch 1 [510/547] Loss: 0.9653 Progress: 93.2%\n",
      "Epoch 1 [520/547] Loss: 1.1833 Progress: 95.1%\n",
      "Epoch 1 [530/547] Loss: 0.8869 Progress: 96.9%\n",
      "Epoch 1 [540/547] Loss: 1.2312 Progress: 98.7%\n",
      "Epoch 1 [547/547] Loss: 0.5722 Progress: 100.0%\n",
      "[Epoch 1] Train Loss: 2.6491 | Time: 1856.82s\n",
      "[Epoch 1] Val Loss: 1.1047\n",
      "—————————————————————— Epoch 2 ——————————————————————\n",
      "Epoch 2 [10/547] Loss: 0.9192 Progress: 1.8%\n",
      "Epoch 2 [20/547] Loss: 1.4665 Progress: 3.7%\n",
      "Epoch 2 [30/547] Loss: 0.7106 Progress: 5.5%\n",
      "Epoch 2 [40/547] Loss: 1.2169 Progress: 7.3%\n",
      "Epoch 2 [50/547] Loss: 1.0213 Progress: 9.1%\n",
      "Epoch 2 [60/547] Loss: 1.1589 Progress: 11.0%\n",
      "Epoch 2 [70/547] Loss: 1.3519 Progress: 12.8%\n",
      "Epoch 2 [80/547] Loss: 0.6992 Progress: 14.6%\n",
      "Epoch 2 [90/547] Loss: 1.0007 Progress: 16.5%\n",
      "Epoch 2 [100/547] Loss: 1.5809 Progress: 18.3%\n",
      "Epoch 2 [110/547] Loss: 0.9538 Progress: 20.1%\n",
      "Epoch 2 [120/547] Loss: 1.5053 Progress: 21.9%\n",
      "Epoch 2 [130/547] Loss: 0.7455 Progress: 23.8%\n",
      "Epoch 2 [140/547] Loss: 1.1184 Progress: 25.6%\n",
      "Epoch 2 [150/547] Loss: 1.1383 Progress: 27.4%\n",
      "Epoch 2 [160/547] Loss: 0.6982 Progress: 29.3%\n",
      "Epoch 2 [170/547] Loss: 0.8154 Progress: 31.1%\n",
      "Epoch 2 [180/547] Loss: 0.7499 Progress: 32.9%\n",
      "Epoch 2 [190/547] Loss: 0.7876 Progress: 34.7%\n",
      "Epoch 2 [200/547] Loss: 0.7436 Progress: 36.6%\n",
      "Epoch 2 [210/547] Loss: 0.8454 Progress: 38.4%\n",
      "Epoch 2 [220/547] Loss: 1.0165 Progress: 40.2%\n",
      "Epoch 2 [230/547] Loss: 1.1555 Progress: 42.0%\n",
      "Epoch 2 [240/547] Loss: 0.9683 Progress: 43.9%\n",
      "Epoch 2 [250/547] Loss: 1.4136 Progress: 45.7%\n",
      "Epoch 2 [260/547] Loss: 0.5589 Progress: 47.5%\n",
      "Epoch 2 [270/547] Loss: 1.0676 Progress: 49.4%\n",
      "Epoch 2 [280/547] Loss: 0.9889 Progress: 51.2%\n",
      "Epoch 2 [290/547] Loss: 1.0548 Progress: 53.0%\n",
      "Epoch 2 [300/547] Loss: 0.8239 Progress: 54.8%\n",
      "Epoch 2 [310/547] Loss: 0.8194 Progress: 56.7%\n",
      "Epoch 2 [320/547] Loss: 0.8955 Progress: 58.5%\n",
      "Epoch 2 [330/547] Loss: 0.9122 Progress: 60.3%\n",
      "Epoch 2 [340/547] Loss: 1.1098 Progress: 62.2%\n",
      "Epoch 2 [350/547] Loss: 0.7828 Progress: 64.0%\n",
      "Epoch 2 [360/547] Loss: 1.6485 Progress: 65.8%\n",
      "Epoch 2 [370/547] Loss: 1.2955 Progress: 67.6%\n",
      "Epoch 2 [380/547] Loss: 1.5053 Progress: 69.5%\n",
      "Epoch 2 [390/547] Loss: 0.9359 Progress: 71.3%\n",
      "Epoch 2 [400/547] Loss: 0.8394 Progress: 73.1%\n",
      "Epoch 2 [410/547] Loss: 0.8321 Progress: 75.0%\n",
      "Epoch 2 [420/547] Loss: 0.9722 Progress: 76.8%\n",
      "Epoch 2 [430/547] Loss: 1.1846 Progress: 78.6%\n",
      "Epoch 2 [440/547] Loss: 0.4571 Progress: 80.4%\n",
      "Epoch 2 [450/547] Loss: 0.8172 Progress: 82.3%\n",
      "Epoch 2 [460/547] Loss: 0.8344 Progress: 84.1%\n",
      "Epoch 2 [470/547] Loss: 1.7394 Progress: 85.9%\n",
      "Epoch 2 [480/547] Loss: 1.4090 Progress: 87.8%\n",
      "Epoch 2 [490/547] Loss: 0.7439 Progress: 89.6%\n",
      "Epoch 2 [500/547] Loss: 1.1029 Progress: 91.4%\n",
      "Epoch 2 [510/547] Loss: 1.0281 Progress: 93.2%\n",
      "Epoch 2 [520/547] Loss: 0.8160 Progress: 95.1%\n",
      "Epoch 2 [530/547] Loss: 0.7849 Progress: 96.9%\n",
      "Epoch 2 [540/547] Loss: 0.9935 Progress: 98.7%\n",
      "Epoch 2 [547/547] Loss: 1.4161 Progress: 100.0%\n",
      "[Epoch 2] Train Loss: 1.0702 | Time: 1570.88s\n",
      "[Epoch 2] Val Loss: 1.0333\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Parent directory saved_models does not exist.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[31], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m model \u001B[38;5;241m=\u001B[39m MMRegressor()\n\u001B[0;32m----> 2\u001B[0m train_and_evaluate(model, train_loader, test_loader, device)\n",
      "Cell \u001B[0;32mIn[30], line 56\u001B[0m, in \u001B[0;36mtrain_and_evaluate\u001B[0;34m(model, train_loader, val_loader, device)\u001B[0m\n\u001B[1;32m     53\u001B[0m     model\u001B[38;5;241m.\u001B[39mtrain()\n\u001B[1;32m     55\u001B[0m \u001B[38;5;66;03m# Save final model\u001B[39;00m\n\u001B[0;32m---> 56\u001B[0m torch\u001B[38;5;241m.\u001B[39msave(model\u001B[38;5;241m.\u001B[39mstate_dict(), model_save_path)\n",
      "File \u001B[0;32m~/miniconda3/lib/python3.12/site-packages/torch/serialization.py:943\u001B[0m, in \u001B[0;36msave\u001B[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001B[0m\n\u001B[1;32m    940\u001B[0m _check_save_filelike(f)\n\u001B[1;32m    942\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m _use_new_zipfile_serialization:\n\u001B[0;32m--> 943\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m _open_zipfile_writer(f) \u001B[38;5;28;01mas\u001B[39;00m opened_zipfile:\n\u001B[1;32m    944\u001B[0m         _save(\n\u001B[1;32m    945\u001B[0m             obj,\n\u001B[1;32m    946\u001B[0m             opened_zipfile,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    949\u001B[0m             _disable_byteorder_record,\n\u001B[1;32m    950\u001B[0m         )\n\u001B[1;32m    951\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m\n",
      "File \u001B[0;32m~/miniconda3/lib/python3.12/site-packages/torch/serialization.py:810\u001B[0m, in \u001B[0;36m_open_zipfile_writer\u001B[0;34m(name_or_buffer)\u001B[0m\n\u001B[1;32m    808\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    809\u001B[0m     container \u001B[38;5;241m=\u001B[39m _open_zipfile_writer_buffer\n\u001B[0;32m--> 810\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m container(name_or_buffer)\n",
      "File \u001B[0;32m~/miniconda3/lib/python3.12/site-packages/torch/serialization.py:781\u001B[0m, in \u001B[0;36m_open_zipfile_writer_file.__init__\u001B[0;34m(self, name)\u001B[0m\n\u001B[1;32m    777\u001B[0m     \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__init__\u001B[39m(\n\u001B[1;32m    778\u001B[0m         torch\u001B[38;5;241m.\u001B[39m_C\u001B[38;5;241m.\u001B[39mPyTorchFileWriter(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfile_stream, _compute_crc32)\n\u001B[1;32m    779\u001B[0m     )\n\u001B[1;32m    780\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 781\u001B[0m     \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__init__\u001B[39m(torch\u001B[38;5;241m.\u001B[39m_C\u001B[38;5;241m.\u001B[39mPyTorchFileWriter(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mname, _compute_crc32))\n",
      "\u001B[0;31mRuntimeError\u001B[0m: Parent directory saved_models does not exist."
     ]
    }
   ],
   "execution_count": 31
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
