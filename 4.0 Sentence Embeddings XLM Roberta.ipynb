{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84eaba44",
   "metadata": {},
   "source": [
    "# Sentence Embeddings - XLM-RoBERTa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84e859b",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f21a33",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab0b320",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from transformers import XLMRobertaTokenizer, XLMRobertaModel, get_cosine_schedule_with_warmup\n",
    "from torch.optim import AdamW\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "from transformers import logging\n",
    "logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a00ed0",
   "metadata": {},
   "source": [
    "### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ad720e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "class Config:\n",
    "    def __init__(self, p):\n",
    "        self.learning_rate = p['learning_rate']\n",
    "        self.epoch = p['epoch']\n",
    "        self.batch_size = p['batch_size']\n",
    "        self.max_len = p['max_len']\n",
    "        self.model_save_path = p['model_save_path']\n",
    "        self.warmup_rate = p['warmup_rate']\n",
    "        self.weight_decay = p['weight_decay']\n",
    "        self.model_pretrain_dir = p['model_pretrain_dir']\n",
    "        self.training_set_path = p['training_set_path']\n",
    "        self.testing_set_path = p['testing_set_path']\n",
    "        self.seed = p['seed']\n",
    "\n",
    "params = {\n",
    "    \"learning_rate\": 2e-5,\n",
    "    \"epoch\": 5,\n",
    "    \"batch_size\": 8,\n",
    "    \"max_len\": 512,\n",
    "    \"model_save_path\": \"best_classifier.pth\",\n",
    "    \"warmup_rate\": 0.1,\n",
    "    \"weight_decay\": 0.01,\n",
    "    \"model_pretrain_dir\": \"xlm-roberta-base\",\n",
    "    \"training_set_path\": \"training_set.csv\",\n",
    "    \"testing_set_path\": \"testing_set.csv\",\n",
    "    \"seed\": 42\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da99b1f1",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77192eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MMClassifier(nn.Module):\n",
    "    def __init__(self, model_path):\n",
    "        super().__init__()\n",
    "        self.backbone = XLMRobertaModel.from_pretrained(model_path)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.classifier = nn.Linear(self.backbone.config.hidden_size, 4)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        output = self.backbone(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        cls_output = output.last_hidden_state[:, 0, :]\n",
    "        return self.classifier(self.dropout(cls_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3febf9e0",
   "metadata": {},
   "source": [
    "### Training + Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19b0b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.tokenizer = XLMRobertaTokenizer.from_pretrained(config.model_pretrain_dir)\n",
    "        set_seed(config.seed)\n",
    "\n",
    "    def dataset(self, path):\n",
    "        print(f\"\\nüìÇ Lade Daten aus: {path}\")\n",
    "        df = pd.read_csv(path, usecols=[\"text1\", \"text2\", \"Overall\"])\n",
    "        df = df.dropna(subset=[\"text1\", \"text2\"])\n",
    "        print(f\"‚úÖ Nach Entfernen von NaNs: {len(df)} Zeilen\")\n",
    "        input_ids, attention_masks, labels = [], [], []\n",
    "\n",
    "        for _, row in df.iterrows():\n",
    "            enc = self.tokenizer(str(row['text1']), str(row['text2']),\n",
    "                               padding='max_length', truncation=True,\n",
    "                               max_length=self.config.max_len, return_tensors='pt')\n",
    "            input_ids.append(enc[\"input_ids\"].squeeze(0))\n",
    "            attention_masks.append(enc[\"attention_mask\"].squeeze(0))\n",
    "            labels.append(int(round(row[\"Overall\"])) - 1)\n",
    "\n",
    "        return torch.stack(input_ids), torch.stack(attention_masks), torch.tensor(labels)\n",
    "\n",
    "    def data_loader(self, ids, masks, labels, shuffle=True):\n",
    "        return DataLoader(TensorDataset(ids, masks, labels), batch_size=self.config.batch_size, shuffle=shuffle)\n",
    "\n",
    "    def evaluate(self, model, loader):\n",
    "        model.eval()\n",
    "        y_true, y_pred = [], []\n",
    "        with torch.no_grad():\n",
    "            for ids, att, y in loader:\n",
    "                ids, att, y = ids.to(self.device), att.to(self.device), y.to(self.device)\n",
    "                logits = model(ids, att)\n",
    "                preds = torch.argmax(logits, dim=1)\n",
    "                y_true.extend(y.tolist())\n",
    "                y_pred.extend(preds.cpu().tolist())\n",
    "        return y_true, y_pred\n",
    "\n",
    "    def train(self):\n",
    "        print(\"üì¶ Lade Trainingsdaten...\")\n",
    "        ids, masks, labels = self.dataset(self.config.training_set_path)\n",
    "        train_loader = self.data_loader(ids, masks, labels)\n",
    "        print(f\"‚úÖ Trainingsloader bereit: {len(train_loader)} Batches\")\n",
    "\n",
    "        print(\"üì¶ Lade Dev/Testdaten...\")\n",
    "        dev_ids, dev_masks, dev_labels = self.dataset(self.config.testing_set_path)\n",
    "        dev_loader = self.data_loader(dev_ids, dev_masks, dev_labels, shuffle=False)\n",
    "        print(f\"‚úÖ Dev/Testloader bereit: {len(dev_loader)} Batches\")\n",
    "\n",
    "        model = MMClassifier(self.config.model_pretrain_dir).to(self.device)\n",
    "        optimizer = AdamW(model.parameters(), lr=self.config.learning_rate, weight_decay=self.config.weight_decay)\n",
    "\n",
    "        total_steps = len(train_loader) * self.config.epoch\n",
    "        scheduler = get_cosine_schedule_with_warmup(optimizer, \n",
    "                                                    num_warmup_steps=int(self.config.warmup_rate * total_steps),\n",
    "                                                    num_training_steps=total_steps)\n",
    "\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        best_acc = 0\n",
    "        train_losses, train_accuracies, dev_accuracies = [], [], []\n",
    "\n",
    "        for epoch in range(self.config.epoch):\n",
    "            print(f\"\\nüöÄ Starte Epoche {epoch + 1}/{self.config.epoch}\")\n",
    "            model.train()\n",
    "            running_loss, correct, total = 0.0, 0, 0\n",
    "\n",
    "            for batch_idx, (ids, att, y) in enumerate(train_loader):\n",
    "                ids, att, y = ids.to(self.device), att.to(self.device), y.to(self.device)\n",
    "                logits = model(ids, att)\n",
    "                loss = criterion(logits, y)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "\n",
    "                running_loss += loss.item()\n",
    "                preds = torch.argmax(logits, dim=1)\n",
    "                correct += (preds == y).sum().item()\n",
    "                total += y.size(0)\n",
    "\n",
    "                if batch_idx % 10 == 0:\n",
    "                    print(f\"  üîÅ Batch {batch_idx + 1}/{len(train_loader)}\")\n",
    "\n",
    "            train_loss = running_loss / len(train_loader)\n",
    "            train_acc = correct / total\n",
    "            y_true_dev, y_pred_dev = self.evaluate(model, dev_loader)\n",
    "            dev_acc = accuracy_score(y_true_dev, y_pred_dev)\n",
    "\n",
    "            train_losses.append(train_loss)\n",
    "            train_accuracies.append(train_acc)\n",
    "            dev_accuracies.append(dev_acc)\n",
    "\n",
    "            print(f\"Epoch {epoch+1} | Train Acc: {train_acc:.4f} | Dev Acc: {dev_acc:.4f} | Loss: {train_loss:.4f}\")\n",
    "\n",
    "            if dev_acc > best_acc:\n",
    "                best_acc = dev_acc\n",
    "                torch.save(model.state_dict(), self.config.model_save_path)\n",
    "\n",
    "        print(f\"\\n‚úÖ Training finished. Best Dev Acc: {best_acc:.4f}\")\n",
    "\n",
    "        # Lernkurve\n",
    "        plt.plot(train_losses, label=\"Train Loss\")\n",
    "        plt.plot(train_accuracies, label=\"Train Accuracy\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Wert\")\n",
    "        plt.title(\"Training Verlauf\")\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        # Beste Modell laden & evaluieren\n",
    "        model.load_state_dict(torch.load(self.config.model_save_path))\n",
    "        model.eval()\n",
    "        y_true, y_pred = self.evaluate(model, dev_loader)\n",
    "        evaluate_classification(y_true, y_pred, description=\"Final Dev Set Evaluation\")\n",
    "\n",
    "def evaluate_classification(y_true, y_pred, description=\"Model\"):\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    prec = precision_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    rec = recall_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "\n",
    "    print(f\"\\nüìä Evaluation ‚Äì {description}\")\n",
    "    print(f\"Accuracy: {acc:.3f}\")\n",
    "    print(f\"Precision: {prec:.3f}\")\n",
    "    print(f\"Recall: {rec:.3f}\")\n",
    "    print(f\"F1 Score: {f1:.3f}\")\n",
    "    print(\"\\n\" + classification_report(y_true, y_pred, digits=3, zero_division=0))\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c225aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config(params)\n",
    "trainer = Trainer(config)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f01f7bd9",
   "metadata": {},
   "source": [
    "## Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42958fb1",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4b4a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from transformers import XLMRobertaTokenizer, XLMRobertaModel, get_cosine_schedule_with_warmup\n",
    "from torch.optim import AdamW\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import json\n",
    "from scipy.stats import pearsonr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be2a8d0",
   "metadata": {},
   "source": [
    "### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5228e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "class Config:\n",
    "    def __init__(self, p):\n",
    "        self.learning_rate = p['learning_rate']\n",
    "        self.epoch = p['epoch']\n",
    "        self.batch_size = p['batch_size']\n",
    "        self.max_len = p['max_len']\n",
    "        self.model_save_path = p['model_save_path']\n",
    "        self.warmup_rate = p['warmup_rate']\n",
    "        self.weight_decay = p['weight_decay']\n",
    "        self.model_pretrain_dir = p['model_pretrain_dir']\n",
    "        self.training_set_path = p['training_set_path']\n",
    "        self.testing_set_path = p['testing_set_path']\n",
    "        self.seed = p['seed']\n",
    "\n",
    "params = {\n",
    "    \"learning_rate\": 2e-5,\n",
    "    \"epoch\": 10,\n",
    "    \"batch_size\": 8,\n",
    "    \"max_len\": 512,\n",
    "    \"model_save_path\": \"/kaggle/working/best_regressor.pth\",\n",
    "    \"warmup_rate\": 0.1,\n",
    "    \"weight_decay\": 0.01,\n",
    "    \"model_pretrain_dir\": \"xlm-roberta-base\",\n",
    "    \"training_set_path\": \"training_set.csv\",\n",
    "    \"testing_set_path\": \"testing_set.csv\",\n",
    "    \"seed\": 42\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aee6bb4",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4688d51a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MMRegressor(nn.Module):\n",
    "    def __init__(self, model_path):\n",
    "        super().__init__()\n",
    "        self.backbone = XLMRobertaModel.from_pretrained(model_path)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.regressor = nn.Linear(self.backbone.config.hidden_size, 1)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        output = self.backbone(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        cls_output = output.last_hidden_state[:, 0, :]\n",
    "        return self.regressor(self.dropout(cls_output)).squeeze(-1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22f8900",
   "metadata": {},
   "source": [
    "### Training + Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bddd06b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.tokenizer = XLMRobertaTokenizer.from_pretrained(config.model_pretrain_dir)\n",
    "        set_seed(config.seed)\n",
    "\n",
    "    def dataset(self, path):\n",
    "        input_ids, attention_masks, labels = [], [], []\n",
    "        df = pd.read_csv(path)\n",
    "        df = df.dropna(subset=[\"text1\", \"text2\", \"Overall\"])\n",
    "        for _, row in df.iterrows():  \n",
    "          enc = self.tokenizer(str(row['text1']), str(row['text2']),\n",
    "                               padding='max_length', truncation=True,\n",
    "                               max_length=self.config.max_len, return_tensors='pt')\n",
    "          input_ids.append(enc['input_ids'].squeeze(0))\n",
    "          attention_masks.append(enc['attention_mask'].squeeze(0))\n",
    "          labels.append(float(row['Overall']))\n",
    "        return torch.stack(input_ids), torch.stack(attention_masks), torch.tensor(labels)\n",
    "\n",
    "    def data_loader(self, ids, masks, labels, shuffle=True):\n",
    "        return DataLoader(TensorDataset(ids, masks, labels),\n",
    "                          batch_size=self.config.batch_size, shuffle=shuffle)\n",
    "\n",
    "    def evaluate(self, model, loader):\n",
    "        model.eval()\n",
    "        preds, targets = [], []\n",
    "        with torch.no_grad():\n",
    "            for ids, att, y in loader:\n",
    "                ids, att, y = ids.to(self.device), att.to(self.device), y.to(self.device)\n",
    "                outputs = model(ids, att)\n",
    "                preds.extend(outputs.cpu().numpy())\n",
    "                targets.extend(y.cpu().numpy())\n",
    "        return np.array(targets), np.array(preds)\n",
    "\n",
    "    def train(self):\n",
    "        print(\"üì¶ Lade Trainingsdaten...\")\n",
    "        ids, masks, labels = self.dataset(self.config.training_set_path)\n",
    "        train_loader = self.data_loader(ids, masks, labels)\n",
    "        print(f\"‚úÖ Trainingsloader bereit: {len(train_loader)} Batches\")\n",
    "\n",
    "        print(\"üì¶ Lade Dev/Testdaten...\")\n",
    "        dev_ids, dev_masks, dev_labels = self.dataset(self.config.testing_set_path)\n",
    "        dev_loader = self.data_loader(dev_ids, dev_masks, dev_labels, shuffle=False)\n",
    "        print(f\"‚úÖ Dev/Testloader bereit: {len(dev_loader)} Batches\")\n",
    "\n",
    "        model = MMRegressor(self.config.model_pretrain_dir).to(self.device)\n",
    "        optimizer = AdamW(model.parameters(), lr=self.config.learning_rate, weight_decay=self.config.weight_decay)\n",
    "        total_steps = len(train_loader) * self.config.epoch\n",
    "        scheduler = get_cosine_schedule_with_warmup(optimizer,\n",
    "                                                    num_warmup_steps=int(self.config.warmup_rate * total_steps),\n",
    "                                                    num_training_steps=total_steps)\n",
    "        criterion = nn.MSELoss()\n",
    "        best_mse = float('inf')\n",
    "\n",
    "        for epoch in range(self.config.epoch):\n",
    "            print(f\"\\nüöÄ Starte Epoche {epoch + 1}/{self.config.epoch}\")\n",
    "            model.train()\n",
    "            total_loss = 0\n",
    "            for batch_idx, (ids, att, y) in enumerate(train_loader):\n",
    "                ids, att, y = ids.to(self.device), att.to(self.device), y.to(self.device)\n",
    "                outputs = model(ids, att)\n",
    "                loss = criterion(outputs, y)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                total_loss += loss.item()\n",
    "                if batch_idx % 10 == 0:\n",
    "                    print(f\"  üîÅ Batch {batch_idx + 1}/{len(train_loader)}\")\n",
    "\n",
    "            y_true_dev, y_pred_dev = self.evaluate(model, dev_loader)\n",
    "            mse = mean_squared_error(y_true_dev, y_pred_dev)\n",
    "\n",
    "            print(f\"Epoch {epoch+1} | Dev MSE: {mse:.4f} | Train Loss: {total_loss:.4f}\")\n",
    "            if mse < best_mse:\n",
    "                best_mse = mse\n",
    "                torch.save(model.state_dict(), self.config.model_save_path)\n",
    "                print(\"üíæ Bestes Modell gespeichert (niedrigstes MSE).\")\n",
    "\n",
    "        print(f\"\\n‚úÖ Training abgeschlossen. Bestes Dev MSE: {best_mse:.4f}\")\n",
    "\n",
    "        model.load_state_dict(torch.load(self.config.model_save_path))\n",
    "        model.eval()\n",
    "        y_true, y_pred = self.evaluate(model, dev_loader)\n",
    "        evaluate_regression(y_true, y_pred, description=\"Final Dev Regression Evaluation\", save_json_file_name=\"/kaggle/working/final_dev_regression_metrics.json\")\n",
    "\n",
    "def evaluate_regression(y_true, y_pred, description=\"Model\", save_json_file_name=None):\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "\n",
    "    try:\n",
    "        pearson_corr, _ = pearsonr(y_true, y_pred)\n",
    "    except Exception:\n",
    "        pearson_corr = float('nan')\n",
    "\n",
    "    print(f\"\\nüìä Regression Evaluation ‚Äì {description}\")\n",
    "    print(f\"Mean Squared Error (MSE): {mse:.3f}\")\n",
    "    print(f\"Mean Absolute Error (MAE): {mae:.3f}\")\n",
    "    print(f\"R2 Score: {r2:.3f}\")\n",
    "    print(f\"Pearson Correlation: {pearson_corr:.3f}\")\n",
    "\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    sns.scatterplot(x=y_true, y=y_pred, alpha=0.6)\n",
    "    plt.plot([min(y_true), max(y_true)], [min(y_true), max(y_true)], '--', color='red')\n",
    "    plt.xlabel(\"True Values\")\n",
    "    plt.ylabel(\"Predictions\")\n",
    "    plt.title(f\"{description} - Regression Prediction Plot\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    results = {\n",
    "        \"Model\": description,\n",
    "        \"MSE\": mse,\n",
    "        \"MAE\": mae,\n",
    "        \"R2\": r2,\n",
    "        \"Pearson\": pearson_corr\n",
    "    }\n",
    "\n",
    "    if save_json_file_name:\n",
    "        os.makedirs(\"results\", exist_ok=True)\n",
    "        path = os.path.join(\"results\", save_json_file_name)\n",
    "        with open(path, \"w\") as f:\n",
    "            json.dump(results, f, indent=2)\n",
    "        print(f\"üìÅ Ergebnisse gespeichert unter: {save_json_file_name}\")\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b4bb15",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config(params)\n",
    "trainer = Trainer(config)\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
