{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2a4f0704",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5c6c42dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: CPU\n"
     ]
    }
   ],
   "source": [
    "# PrÃ¼fe ob GPU verfÃ¼gbar ist\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "print(f\"Using device: {'GPU' if device == 0 else 'CPU'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "375d2f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "\"Share This On:\n",
    "\n",
    "Pin 11 Shares\n",
    "\n",
    "(NEWS ROOM GUYANA) â€” Three persons are currently hospitalized in a serious condition following an accident on the Crabwood Creek Public Road on New Yearâ€™s morning.\n",
    "\n",
    "According to information received, motorcar PNN 7976 driven by 22-year-old Seeram Ramdat was speeding when it collided with a utility pole, injuring the driver and two passengers.\n",
    "\n",
    "The News Room understands that while driving over the Blackwater Creek Bridge, Ramdat lost control of the vehicle which turned turtle and careened about 200 feet away before crashing into the utility pole and coming to a halt on a residentâ€™s bridge.\n",
    "\n",
    "The two occupants, 32-year-old Keron Phillips and 45-year-old Ramnand Kishwar were removed from the wreck in semi-conscious states and rushed to the Skeldon hospital.\n",
    "\n",
    "The driver fled the scene and was subsequently apprehended at his Lot 80 Grant 1718 Crabwood Creek home in a traumatic state. He was also taken to the Skeldon Hospital where he is admitted in a stable condition.\n",
    "\n",
    "The News Room understands that the vehicle is owned by an elderly woman, and Ramdatt took it without her knowledge.\n",
    "\n",
    "Police Commissioner Leslie James on Wednesday disclosed that there has been an 8% increase in road fatalities in 2018.\n",
    "\n",
    "( 0 ) ( 0 )\"\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "max_new_tokens = 128\n",
    "min_length = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45eaa820",
   "metadata": {},
   "source": [
    "# mT5_multilingual_XLSum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5c2fee2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "model_name = \"csebuetnlp/mT5_multilingual_XLSum\"\n",
    "\n",
    "# 3. Tokenizer und Modell laden\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "# 4. Pipeline definieren\n",
    "summarizer = pipeline(\"summarization\", model=model, tokenizer=tokenizer, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d86bb019",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”Ž Zusammenfassung:\n",
      " The BBC's News Room looks at the deaths of three people in a serious condition following an accident on the Crabwood Creek Public Road on New Yearâ€™s morning, which left three people injured and two passengers critically ill. They were taken to hospital in semi-conscious states and rushed to a hospital where they were . .. Warning: This article contains a full transcript of the accident.\n"
     ]
    }
   ],
   "source": [
    "def summarize_article_mt5(text):\n",
    "    # Spezifisches Prepending wie im XLSum Training\n",
    "    formatted_text = \"summarize: \" + text.strip()\n",
    "\n",
    "    input_ids = tokenizer.encode(formatted_text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "    input_ids = input_ids.to(model.device)\n",
    "\n",
    "    output_ids = model.generate(\n",
    "        input_ids,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        min_length=min_length,\n",
    "        num_beams=4,\n",
    "        length_penalty=1.5,  # Optional: Bevorzuge etwas lÃ¤ngere Texte\n",
    "        no_repeat_ngram_size=3,\n",
    "        early_stopping=True,\n",
    "    )\n",
    "\n",
    "    summary = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    return summary\n",
    "\n",
    "summary = summarize_article_mt5(text)\n",
    "print(\"ðŸ”Ž Zusammenfassung:\\n\", summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ee4288b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 16/2736 [16:00<45:21:32, 60.03s/it] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, row \u001b[38;5;129;01min\u001b[39;00m tqdm(df\u001b[38;5;241m.\u001b[39miterrows(), total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(df)):    \n\u001b[0;32m      7\u001b[0m     summary1 \u001b[38;5;241m=\u001b[39m summarize_article_mt5(row[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext1\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m----> 8\u001b[0m     summary2 \u001b[38;5;241m=\u001b[39m \u001b[43msummarize_article_mt5\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m     df\u001b[38;5;241m.\u001b[39mat[idx, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msummary1\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m summary1\n\u001b[0;32m     11\u001b[0m     df\u001b[38;5;241m.\u001b[39mat[idx, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msummary2\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m summary2\n",
      "Cell \u001b[1;32mIn[22], line 8\u001b[0m, in \u001b[0;36msummarize_article_mt5\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m      5\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mencode(formatted_text, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m)\n\u001b[0;32m      6\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m input_ids\u001b[38;5;241m.\u001b[39mto(model\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m----> 8\u001b[0m output_ids \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmin_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmin_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_beams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlength_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Optional: Bevorzuge etwas lÃ¤ngere Texte\u001b[39;49;00m\n\u001b[0;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mno_repeat_ngram_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m summary \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mdecode(output_ids[\u001b[38;5;241m0\u001b[39m], skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m summary\n",
      "File \u001b[1;32mc:\\Users\\laraw\\GitHub\\AI-Lab\\.venv\\lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\laraw\\GitHub\\AI-Lab\\.venv\\lib\\site-packages\\transformers\\generation\\utils.py:2616\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[1;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[0m\n\u001b[0;32m   2609\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[0;32m   2610\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m   2611\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   2612\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[0;32m   2613\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   2614\u001b[0m     )\n\u001b[0;32m   2615\u001b[0m     \u001b[38;5;66;03m# 12. run beam sample\u001b[39;00m\n\u001b[1;32m-> 2616\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_beam_search(\n\u001b[0;32m   2617\u001b[0m         input_ids,\n\u001b[0;32m   2618\u001b[0m         logits_processor\u001b[38;5;241m=\u001b[39mprepared_logits_processor,\n\u001b[0;32m   2619\u001b[0m         stopping_criteria\u001b[38;5;241m=\u001b[39mprepared_stopping_criteria,\n\u001b[0;32m   2620\u001b[0m         generation_config\u001b[38;5;241m=\u001b[39mgeneration_config,\n\u001b[0;32m   2621\u001b[0m         synced_gpus\u001b[38;5;241m=\u001b[39msynced_gpus,\n\u001b[0;32m   2622\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   2623\u001b[0m     )\n\u001b[0;32m   2625\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mGROUP_BEAM_SEARCH:\n\u001b[0;32m   2626\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[0;32m   2627\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[0;32m   2628\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[0;32m   2629\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2635\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[0;32m   2636\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\laraw\\GitHub\\AI-Lab\\.venv\\lib\\site-packages\\transformers\\generation\\utils.py:4046\u001b[0m, in \u001b[0;36mGenerationMixin._beam_search\u001b[1;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[0;32m   4042\u001b[0m logits \u001b[38;5;241m=\u001b[39m model_outputs\u001b[38;5;241m.\u001b[39mlogits[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :]\u001b[38;5;241m.\u001b[39mto(copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32, device\u001b[38;5;241m=\u001b[39minput_ids\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m   4044\u001b[0m \u001b[38;5;66;03m# b. Compute log probs -- get log probabilities from logits, process logits with processors (*e.g.*\u001b[39;00m\n\u001b[0;32m   4045\u001b[0m \u001b[38;5;66;03m# `temperature`, ...), and add new logprobs to existing running logprobs scores.\u001b[39;00m\n\u001b[1;32m-> 4046\u001b[0m log_probs \u001b[38;5;241m=\u001b[39m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunctional\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_softmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4047\u001b[0m log_probs \u001b[38;5;241m=\u001b[39m logits_processor(flat_running_sequences, log_probs)\n\u001b[0;32m   4049\u001b[0m \u001b[38;5;66;03m# Store logits, attentions and hidden_states when required\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\laraw\\GitHub\\AI-Lab\\.venv\\lib\\site-packages\\torch\\nn\\functional.py:2248\u001b[0m, in \u001b[0;36mlog_softmax\u001b[1;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[0;32m   2246\u001b[0m     dim \u001b[38;5;241m=\u001b[39m _get_softmax_dim(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlog_softmax\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim(), _stacklevel)\n\u001b[0;32m   2247\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 2248\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_softmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2249\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2250\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mlog_softmax(dim, dtype\u001b[38;5;241m=\u001b[39mdtype)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "df = pd.read_csv(\"data/full_dataset.csv\")\n",
    "\n",
    "for idx, row in tqdm(df.iterrows(), total=len(df)):    \n",
    "    summary1 = summarize_article_mt5(row[\"text1\"])\n",
    "    summary2 = summarize_article_mt5(row[\"text2\"])\n",
    "\n",
    "    df.at[idx, \"summary1\"] = summary1\n",
    "    df.at[idx, \"summary2\"] = summary2\n",
    "    \n",
    "df.to_csv(\"data/full_dataset_with_summaries.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e251b003",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=128) and `max_length`(=128) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zusammenfassung:  {'summary_text': \"The BBC's weekly The Boss series profiles different content from around the world. This week we speak to Seeram Ramdatt, driver of a motorcar which crashed on New Yearâ€™s morning in the Crabwood Creek Public Road. The News Room explains how he lost control of the vehicle and killed two passengers, including two people, who were involved . Warning: This article contains graphic images.\"}\n"
     ]
    }
   ],
   "source": [
    "prompt = f\"Summarize the contents of the following article in its original language, preserving as much information as possible. Focus on the content.\\n\\n{text}\"\n",
    "\n",
    "summary = summarizer(prompt, max_new_tokens=max_new_tokens, max_length = max_new_tokens, min_length=min_length, do_sample=False)[0]\n",
    "\n",
    "print(\"Zusammenfassung: \", summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2e27ed",
   "metadata": {},
   "source": [
    "## MT5 Sum chunked Summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "39bccd88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\laraw\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "Device set to use cpu\n",
      "Both `max_new_tokens` (=128) and `max_length`(=128) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Your max_length is set to 128, but your input_length is only 117. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=58)\n",
      "Both `max_new_tokens` (=128) and `max_length`(=128) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of people killed in a crash on New Year's Day has risen by almost a third, according to the latest figures from the Department of Motoring and Crime (DMRC) and the Department of Drivers and Drivers (DfD) in England and Wales, which have been linked to an 8% increase in fatalities. . . (Below is a full transcript of a crash in Northamptonshire.)\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
    "import torch\n",
    "import nltk\n",
    "import textwrap\n",
    "\n",
    "# Download Punkt tokenizer\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# 1. Set device\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "\n",
    "# 2. Load summarization model and tokenizer\n",
    "model_name = \"csebuetnlp/mT5_multilingual_XLSum\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "summarizer = pipeline(\"summarization\", model=model, tokenizer=tokenizer, device=device)\n",
    "\n",
    "# 3. Define helper to split long text into chunks of approx. 512 tokens\n",
    "def chunk_text(text, tokenizer, max_tokens=512):\n",
    "    sentences = sent_tokenize(text)\n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "\n",
    "    for sentence in sentences:\n",
    "        candidate = f\"{current_chunk} {sentence}\".strip()\n",
    "        tokenized_len = len(tokenizer.encode(candidate, add_special_tokens=False))\n",
    "        if tokenized_len <= max_tokens:\n",
    "            current_chunk = candidate\n",
    "        else:\n",
    "            chunks.append(current_chunk.strip())\n",
    "            current_chunk = sentence\n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk.strip())\n",
    "\n",
    "    return chunks\n",
    "\n",
    "# 4. Summarize a single long text hierarchically\n",
    "def summarize_hierarchical(text, max_new_tokens=128):\n",
    "    chunks = chunk_text(text, tokenizer)\n",
    "    chunk_summaries = []\n",
    "\n",
    "    for chunk in chunks:\n",
    "        summary = summarizer(\n",
    "            chunk,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            max_length = max_new_tokens,\n",
    "            min_length=min_length,\n",
    "            no_repeat_ngram_size=4,\n",
    "            do_sample=False\n",
    "        )[0]['summary_text']\n",
    "        chunk_summaries.append(summary)\n",
    "\n",
    "    # Meta-summarization step\n",
    "    meta_input = \" \".join(chunk_summaries)\n",
    "    final_summary = summarizer(\n",
    "        meta_input,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        max_length = max_new_tokens,\n",
    "        min_length=min_length,\n",
    "        length_penalty=1.2,\n",
    "        no_repeat_ngram_size=4,\n",
    "        do_sample=False\n",
    "    )[0]['summary_text']\n",
    "\n",
    "    return final_summary\n",
    "\n",
    "\n",
    "# Run hierarchical summarization\n",
    "summary_result = summarize_hierarchical(text)\n",
    "print(summary_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951bd446",
   "metadata": {},
   "source": [
    "# MT5 Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8fad7322",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\laraw\\GitHub\\AI-Lab\\.venv\\lib\\site-packages\\transformers\\convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "# Lade den Summarizer\n",
    "model_name = \"google/mt5-base\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "summarizer = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f2c601ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zusammenfassung:  {'generated_text': '<extra_id_0>, which is a lot more serious, <extra_id_1> is a lot more serious.\" \"Share This On: Pin 11 Shares <extra_id_2>, <extra_id_40> <extra_id_41> <extra_id_41> <extra_id_56>.. \"Pin 11 Shares Share This On: Pin 11 Shares Share This On: Pin 11 Shares \"Share This On: Pin 11 Shares \" Share This On: Pin 11 Shares \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \"'}\n"
     ]
    }
   ],
   "source": [
    "prompt = f\"Summarize: {text}\"\n",
    "\n",
    "summary = summarizer(prompt, max_new_tokens=max_new_tokens, min_length=min_length, do_sample=False)[0]\n",
    "\n",
    "print(\"Zusammenfassung: \", summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f59bc6a8",
   "metadata": {},
   "source": [
    "# Flan T5 Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac445b33",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "# Modellname\n",
    "model_name = \"google/flan-t5-base\"\n",
    "\n",
    "# Tokenizer und Modell laden\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "# Pipeline erstellen\n",
    "summarizer = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c7d86d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zusammenfassung:  {'generated_text': 'Three persons are currently hospitalized in a serious condition following an accident on the Crabwood Creek Public Road on New Yearâ€™s morning. According to information received, motorcar PNN 7976 driven by 22-year-old Seeram Ramdat was speeding when it collided with a utility pole, injuring the driver and two passengers. The News Room understands that while driving over the Blackwater Creek Bridge, Ramdat lost control of the vehicle which turned turtle and careened about 200 feet away before crashing into the utility pole and coming to a halt on a residentâ€™'}\n"
     ]
    }
   ],
   "source": [
    "# prompt = f\"Summarize the following article in its original language, preserving as much information as possible, while being concise. Avoid teaser-like introductions or vague phrases. Shorten the input to about 128 tokens.\\n\\n{text}\"\n",
    "\n",
    "prompt = f\"Summarize the following article as a short and dense summary. Focus only on the facts, avoid introductions or generic phrases. Keep the summary under 100 words.\\n\\n{text}\"\n",
    "\n",
    "summary = summarizer(prompt, max_new_tokens=max_new_tokens, min_length=min_length, do_sample=False)[0]\n",
    "\n",
    "print(\"Zusammenfassung: \", summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d8592c",
   "metadata": {},
   "source": [
    "# M-Bart Large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d8efe4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4777550cab87446dbca54ee3a75c5bc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\laraw\\GitHub\\AI-Lab\\.venv\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\laraw\\.cache\\huggingface\\hub\\models--facebook--mbart-large-cc25. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d04db26e2334e7c9ecc09e01644fff8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.10M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5a9fc56a01945c7be046bd27ff3d1e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.19k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'MBartTokenizer'. \n",
      "The class this function is called from is 'MBart50TokenizerFast'.\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4546f351990e47d492c2eec824d9d542",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/2.44G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee50ed6777b643dabb0d8c9c06528d70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.44G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6093ddc7e45342db885acf5f26d469f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/205 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import MBartForConditionalGeneration, MBart50TokenizerFast, pipeline\n",
    "\n",
    "# Modell laden\n",
    "model_name = \"facebook/mbart-large-cc25\"\n",
    "tokenizer = MBart50TokenizerFast.from_pretrained(model_name)\n",
    "model = MBartForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "summarizer = pipeline(\"summarization\", model=model, tokenizer=tokenizer, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29da118",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zusammenfassung:  ( 0 ) Police Commissioner Police Commissioner Leslie James on Wednesday disclosed that there has been an 8% increase in road fatal fatal fatal fatalities in road fatalities in road fatalities in road fatalities in road fatalities in police. Police. Police. Police. Police. Police. Police. Police. Police. Police. Police, , owned by an 8% increase in an 8% increase in road fatalities in 2018. ( 0 )\"\"\"\"\n"
     ]
    }
   ],
   "source": [
    "def summarize_text_m_bart(text, target_token_count=128):\n",
    "    try:\n",
    "        summary = summarizer(\n",
    "            text,\n",
    "            max_length=target_token_count,\n",
    "            min_length=target_token_count//2,\n",
    "            do_sample=False\n",
    "        )[0][\"summary_text\"]\n",
    "        return summary\n",
    "    except Exception as e:\n",
    "        return f\"[Fehler: {e}]\"\n",
    "\n",
    "\n",
    "summary = summarize_text_m_bart(text)\n",
    "\n",
    "print(\"Zusammenfassung: \", summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f15329",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'MBartTokenizer'. \n",
      "The class this function is called from is 'MBart50TokenizerFast'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zusammenfassung: \"Share This On: Pin 11 Shares (NEWS ROOM GUYANA) â€” Three persons are currently hospitalized in a serious condition following an accident on the Crabwood Creek Public Road on New Yearâ€™s morning. According to information received, motorcar PNN 7976 driven by 22-year-old Seeram Ramdat was speeding when it collided with a utility pole, injuring the driver and two passengers. The News Room understands that while driving over the Blackwater Creek Bridge, Ramdat lost control of the vehicle which turned turtle and careened about 200 feet\n"
     ]
    }
   ],
   "source": [
    "# Modell laden\n",
    "model_name = \"facebook/mbart-large-cc25\"\n",
    "tokenizer = MBart50TokenizerFast.from_pretrained(model_name)\n",
    "model = MBartForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# Sprache setzen (fÃ¼r englischen Text)\n",
    "tokenizer.src_lang = \"en_XX\"\n",
    "\n",
    "def summarize(text, target_lang=\"en_XX\", max_tokens=128):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
    "    \n",
    "    # Optional: Zielsprachen-Token setzen (bei multilingualer Ausgabe)\n",
    "    generated_ids = model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        max_length=max_tokens,\n",
    "        min_length=int(max_tokens * 0.5),\n",
    "        length_penalty=1.0,\n",
    "        num_beams=4,\n",
    "        early_stopping=True,\n",
    "        decoder_start_token_id=tokenizer.lang_code_to_id[target_lang],\n",
    "    )\n",
    "    return tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "\n",
    "summary = summarize(text)\n",
    "print(\"Zusammenfassung:\", summary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
