{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2a4f0704",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5c6c42dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: CPU\n"
     ]
    }
   ],
   "source": [
    "# Pr√ºfe ob GPU verf√ºgbar ist\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "print(f\"Using device: {'GPU' if device == 0 else 'CPU'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "375d2f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "\"Share This On:\n",
    "\n",
    "Pin 11 Shares\n",
    "\n",
    "(NEWS ROOM GUYANA) ‚Äî Three persons are currently hospitalized in a serious condition following an accident on the Crabwood Creek Public Road on New Year‚Äôs morning.\n",
    "\n",
    "According to information received, motorcar PNN 7976 driven by 22-year-old Seeram Ramdat was speeding when it collided with a utility pole, injuring the driver and two passengers.\n",
    "\n",
    "The News Room understands that while driving over the Blackwater Creek Bridge, Ramdat lost control of the vehicle which turned turtle and careened about 200 feet away before crashing into the utility pole and coming to a halt on a resident‚Äôs bridge.\n",
    "\n",
    "The two occupants, 32-year-old Keron Phillips and 45-year-old Ramnand Kishwar were removed from the wreck in semi-conscious states and rushed to the Skeldon hospital.\n",
    "\n",
    "The driver fled the scene and was subsequently apprehended at his Lot 80 Grant 1718 Crabwood Creek home in a traumatic state. He was also taken to the Skeldon Hospital where he is admitted in a stable condition.\n",
    "\n",
    "The News Room understands that the vehicle is owned by an elderly woman, and Ramdatt took it without her knowledge.\n",
    "\n",
    "Police Commissioner Leslie James on Wednesday disclosed that there has been an 8% increase in road fatalities in 2018.\n",
    "\n",
    "( 0 ) ( 0 )\"\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "max_new_tokens = 128\n",
    "min_length = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45eaa820",
   "metadata": {},
   "source": [
    "# mT5_multilingual_XLSum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5c2fee2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\laraw\\GitHub\\AI-Lab\\.venv\\lib\\site-packages\\transformers\\convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "model_name = \"csebuetnlp/mT5_multilingual_XLSum\"\n",
    "\n",
    "# 3. Tokenizer und Modell laden\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "# 4. Pipeline definieren\n",
    "summarizer = pipeline(\"summarization\", model=model, tokenizer=tokenizer, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d86bb019",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîé Zusammenfassung:\n",
      " The BBC's News Room looks at the deaths of three people in a serious condition following an accident on the Crabwood Creek Public Road on New Year‚Äôs morning, which left three people injured and two passengers critically ill. They were taken to hospital in semi-conscious states and rushed to a hospital where they were . .. Warning: This article contains a full transcript of the accident.\n"
     ]
    }
   ],
   "source": [
    "def summarize_article_mt5(text):\n",
    "    # Spezifisches Prepending wie im XLSum Training\n",
    "    formatted_text = \"summarize: \" + text.strip()\n",
    "\n",
    "    input_ids = tokenizer.encode(formatted_text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "    input_ids = input_ids.to(model.device)\n",
    "\n",
    "    output_ids = model.generate(\n",
    "        input_ids,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        min_length=min_length,\n",
    "        num_beams=4,\n",
    "        length_penalty=1.0,  # Optional: Bevorzuge etwas l√§ngere Texte\n",
    "        no_repeat_ngram_size=3,\n",
    "        early_stopping=True,\n",
    "    )\n",
    "\n",
    "    summary = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    return summary\n",
    "\n",
    "summary = summarize_article_mt5(text)\n",
    "print(\"üîé Zusammenfassung:\\n\", summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e251b003",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=128) and `max_length`(=128) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zusammenfassung:  {'summary_text': \"This is a full transcript of the crash on the Crabwood Creek Public Road on New Year‚Äôs morning. Here is the full text of this article which contains information about the accident and why it was written by the BBC's News Room. The BBC News room explains what happened to the driver and two passengers in the early hours of New year.  . ( Warning : This article has been published without any further information )\"}\n"
     ]
    }
   ],
   "source": [
    "prompt = f\"Summarize the contents of the following article in its original language, preserving as much information as possible. Avoid filler phrases like 'Here is the full text'. Focus on the content.\\n\\n{text}\"\n",
    "\n",
    "summary = summarizer(prompt, max_new_tokens=max_new_tokens, max_length = max_new_tokens, min_length=min_length, do_sample=False)[0]\n",
    "\n",
    "print(\"Zusammenfassung: \", summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2e27ed",
   "metadata": {},
   "source": [
    "## MT5 Sum chunked Summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "39bccd88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\laraw\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "Device set to use cpu\n",
      "Both `max_new_tokens` (=128) and `max_length`(=128) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Your max_length is set to 128, but your input_length is only 117. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=58)\n",
      "Both `max_new_tokens` (=128) and `max_length`(=128) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of people killed in a crash on New Year's Day has risen by almost a third, according to the latest figures from the Department of Motoring and Crime (DMRC) and the Department of Drivers and Drivers (DfD) in England and Wales, which have been linked to an 8% increase in fatalities. . . (Below is a full transcript of a crash in Northamptonshire.)\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
    "import torch\n",
    "import nltk\n",
    "import textwrap\n",
    "\n",
    "# Download Punkt tokenizer\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# 1. Set device\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "\n",
    "# 2. Load summarization model and tokenizer\n",
    "model_name = \"csebuetnlp/mT5_multilingual_XLSum\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "summarizer = pipeline(\"summarization\", model=model, tokenizer=tokenizer, device=device)\n",
    "\n",
    "# 3. Define helper to split long text into chunks of approx. 512 tokens\n",
    "def chunk_text(text, tokenizer, max_tokens=512):\n",
    "    sentences = sent_tokenize(text)\n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "\n",
    "    for sentence in sentences:\n",
    "        candidate = f\"{current_chunk} {sentence}\".strip()\n",
    "        tokenized_len = len(tokenizer.encode(candidate, add_special_tokens=False))\n",
    "        if tokenized_len <= max_tokens:\n",
    "            current_chunk = candidate\n",
    "        else:\n",
    "            chunks.append(current_chunk.strip())\n",
    "            current_chunk = sentence\n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk.strip())\n",
    "\n",
    "    return chunks\n",
    "\n",
    "# 4. Summarize a single long text hierarchically\n",
    "def summarize_hierarchical(text, max_new_tokens=128):\n",
    "    chunks = chunk_text(text, tokenizer)\n",
    "    chunk_summaries = []\n",
    "\n",
    "    for chunk in chunks:\n",
    "        summary = summarizer(\n",
    "            chunk,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            max_length = max_new_tokens,\n",
    "            min_length=min_length,\n",
    "            no_repeat_ngram_size=4,\n",
    "            do_sample=False\n",
    "        )[0]['summary_text']\n",
    "        chunk_summaries.append(summary)\n",
    "\n",
    "    # Meta-summarization step\n",
    "    meta_input = \" \".join(chunk_summaries)\n",
    "    final_summary = summarizer(\n",
    "        meta_input,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        max_length = max_new_tokens,\n",
    "        min_length=min_length,\n",
    "        length_penalty=1.2,\n",
    "        no_repeat_ngram_size=4,\n",
    "        do_sample=False\n",
    "    )[0]['summary_text']\n",
    "\n",
    "    return final_summary\n",
    "\n",
    "\n",
    "# Run hierarchical summarization\n",
    "summary_result = summarize_hierarchical(text)\n",
    "print(summary_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951bd446",
   "metadata": {},
   "source": [
    "# MT5 Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8fad7322",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "# Lade den Summarizer\n",
    "model_name = \"google/mt5-base\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "summarizer = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f2c601ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zusammenfassung:  {'generated_text': '<extra_id_0> the following paragraphs in a sentence: <extra_id_1>. Please note that this is not a grammar hazard. <extra_id_2>. <extra_id_17> <extra_id_18> <extra_id_19> <extra_id_20> <extra_id_22> <extra_id_22> <extra_id_22>r <extra_id_56>.com: <extra_id_56>.guyana@ <extra_id_20>a <extra_id_20> <extra_id_16>guy <extra_id_20>a <extra_id_20> <extra_id_16>guy <extra_id_4> <extra_id_56>  <extra_id_56> \"Crbwood Creek Road accidents\" Related Articles  <extra_id_39> <extra_id_39> <extra_id_39> <extra_id_39> <extra_id_39> <extra_id_39> <extra_id_4>·àç·ç°·ç° <extra_id_39>t <extra_id_39> Previous <extra_id_39> Previous <extra_id_39> <extra_id_39> <extra_id_39> <extra_id_39> <extra_id_39> <extra_id_39> <extra_id_39> <extra_id_39> <extra_id_39> <extra_id_39> <extra_id_39> <extra_id_39> <extra_id_39> <extra_id_39> <extra_id_39> <extra_id_39> <extra_id_39> <extra_id_39> <extra_id_39> <extra_id_39> <extra_id_39> <extra_id_39> <extra_id_39> <extra_id_39>'}\n"
     ]
    }
   ],
   "source": [
    "prompt = f\"Summarize the following article in its original language, preserving as much information as possible. Avoid teaser-like introductions or vague phrases.\\n\\n{text}\"\n",
    "\n",
    "summary = summarizer(prompt, max_new_tokens=max_new_tokens, min_length=min_length, do_sample=False)[0]\n",
    "\n",
    "print(\"Zusammenfassung: \", summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f59bc6a8",
   "metadata": {},
   "source": [
    "# Flan T5 Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac445b33",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "# Modellname\n",
    "model_name = \"google/flan-t5-base\"\n",
    "\n",
    "# Tokenizer und Modell laden\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "# Pipeline erstellen\n",
    "summarizer = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c7d86d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zusammenfassung:  {'generated_text': 'Three persons are currently hospitalized in a serious condition following an accident on the Crabwood Creek Public Road on New Year‚Äôs morning. According to information received, motorcar PNN 7976 driven by 22-year-old Seeram Ramdat was speeding when it collided with a utility pole, injuring the driver and two passengers. The News Room understands that while driving over the Blackwater Creek Bridge, Ramdat lost control of the vehicle which turned turtle and careened about 200 feet away before crashing into the utility pole and coming to a halt on a resident‚Äô'}\n"
     ]
    }
   ],
   "source": [
    "# prompt = f\"Summarize the following article in its original language, preserving as much information as possible, while being concise. Avoid teaser-like introductions or vague phrases. Shorten the input to about 128 tokens.\\n\\n{text}\"\n",
    "\n",
    "prompt = f\"Summarize the following article as a short and dense summary. Focus only on the facts, avoid introductions or generic phrases. Keep the summary under 100 words.\\n\\n{text}\"\n",
    "\n",
    "summary = summarizer(prompt, max_new_tokens=max_new_tokens, min_length=min_length, do_sample=False)[0]\n",
    "\n",
    "print(\"Zusammenfassung: \", summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d8592c",
   "metadata": {},
   "source": [
    "# M-Bart Large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d8efe4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4777550cab87446dbca54ee3a75c5bc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\laraw\\GitHub\\AI-Lab\\.venv\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\laraw\\.cache\\huggingface\\hub\\models--facebook--mbart-large-cc25. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d04db26e2334e7c9ecc09e01644fff8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.10M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5a9fc56a01945c7be046bd27ff3d1e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.19k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'MBartTokenizer'. \n",
      "The class this function is called from is 'MBart50TokenizerFast'.\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4546f351990e47d492c2eec824d9d542",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/2.44G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee50ed6777b643dabb0d8c9c06528d70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.44G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6093ddc7e45342db885acf5f26d469f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/205 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import MBartForConditionalGeneration, MBart50TokenizerFast, pipeline\n",
    "\n",
    "# Modell laden\n",
    "model_name = \"facebook/mbart-large-cc25\"\n",
    "tokenizer = MBart50TokenizerFast.from_pretrained(model_name)\n",
    "model = MBartForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "summarizer = pipeline(\"summarization\", model=model, tokenizer=tokenizer, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29da118",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zusammenfassung:  ( 0 ) Police Commissioner Police Commissioner Leslie James on Wednesday disclosed that there has been an 8% increase in road fatal fatal fatal fatalities in road fatalities in road fatalities in road fatalities in road fatalities in police. Police. Police. Police. Police. Police. Police. Police. Police. Police. Police, , owned by an 8% increase in an 8% increase in road fatalities in 2018. ( 0 )\"\"\"\"\n"
     ]
    }
   ],
   "source": [
    "def summarize_text_m_bart(text, target_token_count=128):\n",
    "    try:\n",
    "        summary = summarizer(\n",
    "            text,\n",
    "            max_length=target_token_count,\n",
    "            min_length=target_token_count//2,\n",
    "            do_sample=False\n",
    "        )[0][\"summary_text\"]\n",
    "        return summary\n",
    "    except Exception as e:\n",
    "        return f\"[Fehler: {e}]\"\n",
    "\n",
    "\n",
    "summary = summarize_text_m_bart(text)\n",
    "\n",
    "print(\"Zusammenfassung: \", summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f15329",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'MBartTokenizer'. \n",
      "The class this function is called from is 'MBart50TokenizerFast'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zusammenfassung: \"Share This On: Pin 11 Shares (NEWS ROOM GUYANA) ‚Äî Three persons are currently hospitalized in a serious condition following an accident on the Crabwood Creek Public Road on New Year‚Äôs morning. According to information received, motorcar PNN 7976 driven by 22-year-old Seeram Ramdat was speeding when it collided with a utility pole, injuring the driver and two passengers. The News Room understands that while driving over the Blackwater Creek Bridge, Ramdat lost control of the vehicle which turned turtle and careened about 200 feet\n"
     ]
    }
   ],
   "source": [
    "# Modell laden\n",
    "model_name = \"facebook/mbart-large-cc25\"\n",
    "tokenizer = MBart50TokenizerFast.from_pretrained(model_name)\n",
    "model = MBartForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# Sprache setzen (f√ºr englischen Text)\n",
    "tokenizer.src_lang = \"en_XX\"\n",
    "\n",
    "def summarize(text, target_lang=\"en_XX\", max_tokens=128):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
    "    \n",
    "    # Optional: Zielsprachen-Token setzen (bei multilingualer Ausgabe)\n",
    "    generated_ids = model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        max_length=max_tokens,\n",
    "        min_length=int(max_tokens * 0.5),\n",
    "        length_penalty=1.0,\n",
    "        num_beams=4,\n",
    "        early_stopping=True,\n",
    "        decoder_start_token_id=tokenizer.lang_code_to_id[target_lang],\n",
    "    )\n",
    "    return tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "\n",
    "summary = summarize(text)\n",
    "print(\"Zusammenfassung:\", summary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
