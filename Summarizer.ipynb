{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2a4f0704",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5c6c42dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: GPU\n"
     ]
    }
   ],
   "source": [
    "# PrÃ¼fe ob GPU verfÃ¼gbar ist\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "print(f\"Using device: {'GPU' if device == 0 else 'CPU'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "375d2f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "\"Share This On:\n",
    "\n",
    "Pin 11 Shares\n",
    "\n",
    "(NEWS ROOM GUYANA) â€” Three persons are currently hospitalized in a serious condition following an accident on the Crabwood Creek Public Road on New Yearâ€™s morning.\n",
    "\n",
    "According to information received, motorcar PNN 7976 driven by 22-year-old Seeram Ramdat was speeding when it collided with a utility pole, injuring the driver and two passengers.\n",
    "\n",
    "The News Room understands that while driving over the Blackwater Creek Bridge, Ramdat lost control of the vehicle which turned turtle and careened about 200 feet away before crashing into the utility pole and coming to a halt on a residentâ€™s bridge.\n",
    "\n",
    "The two occupants, 32-year-old Keron Phillips and 45-year-old Ramnand Kishwar were removed from the wreck in semi-conscious states and rushed to the Skeldon hospital.\n",
    "\n",
    "The driver fled the scene and was subsequently apprehended at his Lot 80 Grant 1718 Crabwood Creek home in a traumatic state. He was also taken to the Skeldon Hospital where he is admitted in a stable condition.\n",
    "\n",
    "The News Room understands that the vehicle is owned by an elderly woman, and Ramdatt took it without her knowledge.\n",
    "\n",
    "Police Commissioner Leslie James on Wednesday disclosed that there has been an 8% increase in road fatalities in 2018.\n",
    "\n",
    "( 0 ) ( 0 )\"\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "max_new_tokens = 128\n",
    "min_length = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45eaa820",
   "metadata": {},
   "source": [
    "# mT5_multilingual_XLSum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5c2fee2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "v:\\Lara\\GitHub\\AI-Lab\\.venv\\lib\\site-packages\\transformers\\convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "model_name = \"csebuetnlp/mT5_multilingual_XLSum\"\n",
    "\n",
    "# 3. Tokenizer und Modell laden\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "# 4. Pipeline definieren\n",
    "summarizer = pipeline(\"summarization\", model=model, tokenizer=tokenizer, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d86bb019",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”Ž Zusammenfassung:\n",
      " The BBC's News Room looks at the deaths of three people in a serious condition following an accident on the Crabwood Creek Public Road on New Yearâ€™s morning, which left three people injured and two passengers critically ill. They were taken to hospital in semi-conscious states and rushed to a hospital where they were . .. Warning: This article contains a full transcript of the accident.\n"
     ]
    }
   ],
   "source": [
    "def summarize_article_mt5(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"  # oder z.â€¯B. \"N/A\"\n",
    "\n",
    "    # Spezifisches Prepending wie im XLSum Training\n",
    "    formatted_text = \"summarize: \" + text.strip()\n",
    "\n",
    "    input_ids = tokenizer.encode(formatted_text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "    input_ids = input_ids.to(model.device)\n",
    "\n",
    "    output_ids = model.generate(\n",
    "        input_ids,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        min_length=min_length,\n",
    "        num_beams=4,\n",
    "        length_penalty=1.0,  # Optional: Bevorzuge etwas lÃ¤ngere Texte\n",
    "        no_repeat_ngram_size=3,\n",
    "        early_stopping=True,\n",
    "    )\n",
    "\n",
    "    summary = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    return summary\n",
    "\n",
    "summary = summarize_article_mt5(text)\n",
    "print(\"ðŸ”Ž Zusammenfassung:\\n\", summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4288b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2736/2736 [4:23:17<00:00,  5.77s/it]  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "df = pd.read_csv(\"data/full_dataset.csv\")\n",
    "\n",
    "for idx, row in tqdm(df.iterrows(), total=len(df)):    \n",
    "    summary1 = summarize_article_mt5(row[\"text1\"])\n",
    "    summary2 = summarize_article_mt5(row[\"text2\"])\n",
    "\n",
    "    df.at[idx, \"summary1\"] = summary1\n",
    "    df.at[idx, \"summary2\"] = summary2\n",
    "    \n",
    "df.to_csv(\"data/train_dataset_with_summaries.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e251b003",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=128) and `max_length`(=128) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zusammenfassung:  {'summary_text': \"The BBC's weekly The Boss series profiles different content from around the world. This week we speak to Seeram Ramdatt, driver of a motorcar which crashed on New Yearâ€™s morning in the Crabwood Creek Public Road. The News Room explains how he lost control of the vehicle and killed two passengers, including two people, who were involved . Warning: This article contains graphic images.\"}\n"
     ]
    }
   ],
   "source": [
    "prompt = f\"Summarize the contents of the following article in its original language, preserving as much information as possible. Focus on the content.\\n\\n{text}\"\n",
    "\n",
    "summary = summarizer(prompt, max_new_tokens=max_new_tokens, max_length = max_new_tokens, min_length=min_length, do_sample=False)[0]\n",
    "\n",
    "print(\"Zusammenfassung: \", summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2e27ed",
   "metadata": {},
   "source": [
    "## MT5 Sum chunked Summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "39bccd88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Lara\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "v:\\Lara\\GitHub\\AI-Lab\\.venv\\lib\\site-packages\\transformers\\convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Lara/nltk_data'\n    - 'v:\\\\Lara\\\\GitHub\\\\AI-Lab\\\\.venv\\\\nltk_data'\n    - 'v:\\\\Lara\\\\GitHub\\\\AI-Lab\\\\.venv\\\\share\\\\nltk_data'\n    - 'v:\\\\Lara\\\\GitHub\\\\AI-Lab\\\\.venv\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Lara\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 70\u001b[0m\n\u001b[0;32m     66\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m final_summary\n\u001b[0;32m     69\u001b[0m \u001b[38;5;66;03m# Run hierarchical summarization\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m summary_result \u001b[38;5;241m=\u001b[39m \u001b[43msummarize_hierarchical\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28mprint\u001b[39m(summary_result)\n",
      "Cell \u001b[1;32mIn[20], line 40\u001b[0m, in \u001b[0;36msummarize_hierarchical\u001b[1;34m(text, max_new_tokens)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msummarize_hierarchical\u001b[39m(text, max_new_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m):\n\u001b[1;32m---> 40\u001b[0m     chunks \u001b[38;5;241m=\u001b[39m \u001b[43mchunk_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     41\u001b[0m     chunk_summaries \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     43\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m chunks:\n",
      "Cell \u001b[1;32mIn[20], line 21\u001b[0m, in \u001b[0;36mchunk_text\u001b[1;34m(text, tokenizer, max_tokens)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mchunk_text\u001b[39m(text, tokenizer, max_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m):\n\u001b[1;32m---> 21\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m     chunks \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     23\u001b[0m     current_chunk \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32mv:\\Lara\\GitHub\\AI-Lab\\.venv\\lib\\site-packages\\nltk\\tokenize\\__init__.py:119\u001b[0m, in \u001b[0;36msent_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msent_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    110\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;124;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;124;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 119\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43m_get_punkt_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
      "File \u001b[1;32mv:\\Lara\\GitHub\\AI-Lab\\.venv\\lib\\site-packages\\nltk\\tokenize\\__init__.py:105\u001b[0m, in \u001b[0;36m_get_punkt_tokenizer\u001b[1;34m(language)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mlru_cache\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_get_punkt_tokenizer\u001b[39m(language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     98\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;124;03m    A constructor for the PunktTokenizer that utilizes\u001b[39;00m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;124;03m    a lru cache for performance.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;124;03m    :type language: str\u001b[39;00m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPunktTokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mv:\\Lara\\GitHub\\AI-Lab\\.venv\\lib\\site-packages\\nltk\\tokenize\\punkt.py:1744\u001b[0m, in \u001b[0;36mPunktTokenizer.__init__\u001b[1;34m(self, lang)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m   1743\u001b[0m     PunktSentenceTokenizer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m-> 1744\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_lang\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mv:\\Lara\\GitHub\\AI-Lab\\.venv\\lib\\site-packages\\nltk\\tokenize\\punkt.py:1749\u001b[0m, in \u001b[0;36mPunktTokenizer.load_lang\u001b[1;34m(self, lang)\u001b[0m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mload_lang\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m   1747\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m find\n\u001b[1;32m-> 1749\u001b[0m     lang_dir \u001b[38;5;241m=\u001b[39m \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtokenizers/punkt_tab/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlang\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1750\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_params \u001b[38;5;241m=\u001b[39m load_punkt_params(lang_dir)\n\u001b[0;32m   1751\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lang \u001b[38;5;241m=\u001b[39m lang\n",
      "File \u001b[1;32mv:\\Lara\\GitHub\\AI-Lab\\.venv\\lib\\site-packages\\nltk\\data.py:579\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    577\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[0;32m    578\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 579\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Lara/nltk_data'\n    - 'v:\\\\Lara\\\\GitHub\\\\AI-Lab\\\\.venv\\\\nltk_data'\n    - 'v:\\\\Lara\\\\GitHub\\\\AI-Lab\\\\.venv\\\\share\\\\nltk_data'\n    - 'v:\\\\Lara\\\\GitHub\\\\AI-Lab\\\\.venv\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Lara\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
    "import torch\n",
    "import nltk\n",
    "import textwrap\n",
    "\n",
    "# Download Punkt tokenizer\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# 1. Set device\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "\n",
    "# 2. Load summarization model and tokenizer\n",
    "model_name = \"csebuetnlp/mT5_multilingual_XLSum\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "summarizer = pipeline(\"summarization\", model=model, tokenizer=tokenizer, device=device)\n",
    "\n",
    "# 3. Define helper to split long text into chunks of approx. 512 tokens\n",
    "def chunk_text(text, tokenizer, max_tokens=512):\n",
    "    sentences = sent_tokenize(text)\n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "\n",
    "    for sentence in sentences:\n",
    "        candidate = f\"{current_chunk} {sentence}\".strip()\n",
    "        tokenized_len = len(tokenizer.encode(candidate, add_special_tokens=False))\n",
    "        if tokenized_len <= max_tokens:\n",
    "            current_chunk = candidate\n",
    "        else:\n",
    "            chunks.append(current_chunk.strip())\n",
    "            current_chunk = sentence\n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk.strip())\n",
    "\n",
    "    return chunks\n",
    "\n",
    "# 4. Summarize a single long text hierarchically\n",
    "def summarize_hierarchical(text, max_new_tokens=128):\n",
    "    chunks = chunk_text(text, tokenizer)\n",
    "    chunk_summaries = []\n",
    "\n",
    "    for chunk in chunks:\n",
    "        summary = summarizer(\n",
    "            chunk,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            max_length = max_new_tokens,\n",
    "            min_length=min_length,\n",
    "            no_repeat_ngram_size=4,\n",
    "            do_sample=False\n",
    "        )[0]['summary_text']\n",
    "        chunk_summaries.append(summary)\n",
    "\n",
    "    # Meta-summarization step\n",
    "    meta_input = \" \".join(chunk_summaries)\n",
    "    final_summary = summarizer(\n",
    "        meta_input,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        max_length = max_new_tokens,\n",
    "        min_length=min_length,\n",
    "        length_penalty=1.2,\n",
    "        no_repeat_ngram_size=4,\n",
    "        do_sample=False\n",
    "    )[0]['summary_text']\n",
    "\n",
    "    return final_summary\n",
    "\n",
    "\n",
    "# Run hierarchical summarization\n",
    "summary_result = summarize_hierarchical(text)\n",
    "print(summary_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951bd446",
   "metadata": {},
   "source": [
    "# MT5 Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fad7322",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\laraw\\GitHub\\AI-Lab\\.venv\\lib\\site-packages\\transformers\\convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "# Lade den Summarizer\n",
    "model_name = \"google/mt5-base\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "summarizer = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c601ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zusammenfassung:  {'generated_text': '<extra_id_0>, which is a lot more serious, <extra_id_1> is a lot more serious.\" \"Share This On: Pin 11 Shares <extra_id_2>, <extra_id_40> <extra_id_41> <extra_id_41> <extra_id_56>.. \"Pin 11 Shares Share This On: Pin 11 Shares Share This On: Pin 11 Shares \"Share This On: Pin 11 Shares \" Share This On: Pin 11 Shares \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \"'}\n"
     ]
    }
   ],
   "source": [
    "prompt = f\"Summarize: {text}\"\n",
    "\n",
    "summary = summarizer(prompt, max_new_tokens=max_new_tokens, min_length=min_length, do_sample=False)[0]\n",
    "\n",
    "print(\"Zusammenfassung: \", summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f59bc6a8",
   "metadata": {},
   "source": [
    "# Flan T5 Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac445b33",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "# Modellname\n",
    "model_name = \"google/flan-t5-base\"\n",
    "\n",
    "# Tokenizer und Modell laden\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "# Pipeline erstellen\n",
    "summarizer = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c7d86d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zusammenfassung:  {'generated_text': 'Three persons are currently hospitalized in a serious condition following an accident on the Crabwood Creek Public Road on New Yearâ€™s morning. According to information received, motorcar PNN 7976 driven by 22-year-old Seeram Ramdat was speeding when it collided with a utility pole, injuring the driver and two passengers. The News Room understands that while driving over the Blackwater Creek Bridge, Ramdat lost control of the vehicle which turned turtle and careened about 200 feet away before crashing into the utility pole and coming to a halt on a residentâ€™'}\n"
     ]
    }
   ],
   "source": [
    "# prompt = f\"Summarize the following article in its original language, preserving as much information as possible, while being concise. Avoid teaser-like introductions or vague phrases. Shorten the input to about 128 tokens.\\n\\n{text}\"\n",
    "\n",
    "prompt = f\"Summarize the following article as a short and dense summary. Focus only on the facts, avoid introductions or generic phrases. Keep the summary under 100 words.\\n\\n{text}\"\n",
    "\n",
    "summary = summarizer(prompt, max_new_tokens=max_new_tokens, min_length=min_length, do_sample=False)[0]\n",
    "\n",
    "print(\"Zusammenfassung: \", summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d8592c",
   "metadata": {},
   "source": [
    "# M-Bart Large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d8efe4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4777550cab87446dbca54ee3a75c5bc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\laraw\\GitHub\\AI-Lab\\.venv\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\laraw\\.cache\\huggingface\\hub\\models--facebook--mbart-large-cc25. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d04db26e2334e7c9ecc09e01644fff8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.10M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5a9fc56a01945c7be046bd27ff3d1e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.19k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'MBartTokenizer'. \n",
      "The class this function is called from is 'MBart50TokenizerFast'.\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4546f351990e47d492c2eec824d9d542",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/2.44G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee50ed6777b643dabb0d8c9c06528d70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.44G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6093ddc7e45342db885acf5f26d469f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/205 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import MBartForConditionalGeneration, MBart50TokenizerFast, pipeline\n",
    "\n",
    "# Modell laden\n",
    "model_name = \"facebook/mbart-large-cc25\"\n",
    "tokenizer = MBart50TokenizerFast.from_pretrained(model_name)\n",
    "model = MBartForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "summarizer = pipeline(\"summarization\", model=model, tokenizer=tokenizer, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29da118",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zusammenfassung:  ( 0 ) Police Commissioner Police Commissioner Leslie James on Wednesday disclosed that there has been an 8% increase in road fatal fatal fatal fatalities in road fatalities in road fatalities in road fatalities in road fatalities in police. Police. Police. Police. Police. Police. Police. Police. Police. Police. Police, , owned by an 8% increase in an 8% increase in road fatalities in 2018. ( 0 )\"\"\"\"\n"
     ]
    }
   ],
   "source": [
    "def summarize_text_m_bart(text, target_token_count=128):\n",
    "    try:\n",
    "        summary = summarizer(\n",
    "            text,\n",
    "            max_length=target_token_count,\n",
    "            min_length=target_token_count//2,\n",
    "            do_sample=False\n",
    "        )[0][\"summary_text\"]\n",
    "        return summary\n",
    "    except Exception as e:\n",
    "        return f\"[Fehler: {e}]\"\n",
    "\n",
    "\n",
    "summary = summarize_text_m_bart(text)\n",
    "\n",
    "print(\"Zusammenfassung: \", summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f15329",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'MBartTokenizer'. \n",
      "The class this function is called from is 'MBart50TokenizerFast'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zusammenfassung: \"Share This On: Pin 11 Shares (NEWS ROOM GUYANA) â€” Three persons are currently hospitalized in a serious condition following an accident on the Crabwood Creek Public Road on New Yearâ€™s morning. According to information received, motorcar PNN 7976 driven by 22-year-old Seeram Ramdat was speeding when it collided with a utility pole, injuring the driver and two passengers. The News Room understands that while driving over the Blackwater Creek Bridge, Ramdat lost control of the vehicle which turned turtle and careened about 200 feet\n"
     ]
    }
   ],
   "source": [
    "# Modell laden\n",
    "model_name = \"facebook/mbart-large-cc25\"\n",
    "tokenizer = MBart50TokenizerFast.from_pretrained(model_name)\n",
    "model = MBartForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# Sprache setzen (fÃ¼r englischen Text)\n",
    "tokenizer.src_lang = \"en_XX\"\n",
    "\n",
    "def summarize(text, target_lang=\"en_XX\", max_tokens=128):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
    "    \n",
    "    # Optional: Zielsprachen-Token setzen (bei multilingualer Ausgabe)\n",
    "    generated_ids = model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        max_length=max_tokens,\n",
    "        min_length=int(max_tokens * 0.5),\n",
    "        length_penalty=1.0,\n",
    "        num_beams=4,\n",
    "        early_stopping=True,\n",
    "        decoder_start_token_id=tokenizer.lang_code_to_id[target_lang],\n",
    "    )\n",
    "    return tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "\n",
    "summary = summarize(text)\n",
    "print(\"Zusammenfassung:\", summary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
